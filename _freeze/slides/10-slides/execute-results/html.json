{
  "hash": "e2907a492b8a42d2f39a94d9433890f4",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Scientific Publishing and the Replication Crisis\"\nformat: \n  revealjs:\n    theme: simple\n    smaller: true\n    slide-number: true\n    # incremental: true\n    # code-link: true\n    chalkboard: true\n    history: false\n---\n\n::: {.cell}\n\n:::\n\n\n\n## Overview\n\n::::: columns\n::: {.column style=\"font-size: smaller;\"}\n1.  **On the whole science works**\n\n2. **Replication Crisis**\n\n3. **Reproducibility**\n\n4. **Replicability**\n    - P-Hacking\n    - File-drawer effect\n    \n:::\n\n::: {.column style=\"font-size: smaller;\"}\n\n5. **How to do better science?**\n    - Reproducible manuscripts\n    - Preregistrations\n\n:::\n:::::\n\n# On the whole science works\n\n---\n\n## {background-color=\"black\"}\n\n![](images/newton.jpg){.absolute top=0 right=0 height=\"100%\"}\n\n::: {.absolute left=\"0%\" top=\"50%\" style=\"font-size:1.5em; padding: 0.5em 1em; background-color: rgba(255, 255, 255, .5); backdrop-filter: blur(5px); box-shadow: 0 0 1rem 0 rgba(0, 0, 0, .5); border-radius: 5px;\"}\nIf I have seen further than others, \n\nit is by standing upon the shoulders \n\nof giants.  \n<span style=\"display:block; font-size:0.6em; font-style:italic; margin-top:0.5em; text-align:right;\">‚Äî Isaac Newton</span>\n:::\n\n---\n\n## Science is cumulative\n\n<br> \n\n- Researchers build on each others findings\n\n<br>\n\n- This way, science is self correcting: \n  If something doesn't doesn't turn out to be right, (at some point) other researchers will notice\n\n\n---\n\n## (A typical textbook) pyramid of evidence\n\n![](images/pyramid_of_evidence.png){.center width=\"100%\"}\n\n---\n\n<br>\n\n... but as great as science often is, it has some issues, and some are very structural\n\n\n# The replication Crisis\n\n---\n\n![](images/economist-headline.png){fig-align=\"center\" width=\"100%\"}\n\n---\n\n## Reproducibility\n\n<br>\n\nWhen scientists can obtain the results of other scientists...\n\n<br>\n\n... **using the same data and following the same methods**\n\n. . .\n\n<br>\nWould this ever not be the case?\n\n. . .\n\n<br>\nUnfortunately, yes!\n\n---\n\n## \n\n![[Oza, A. (2023). Reproducibility trial: 246 biologists get different results from same data sets. Nature.](https://www.nature.com/articles/d41586-023-03177-1)](images/reproducibility_nature.png)\n\n---\n\n## Replicability\n\n<br>\n\nWhen scientists can obtain the results of other scientists...\n\n<br>\n\n... **collecting new data, following the same methods**\n\n---\n\n## \n\n![[OPEN SCIENCE COLLABORATION. (2015). Estimating the reproducibility of psychological science. Science.](https://doi.org/10.1126/science.aac4716)](images/psychology-replication.png)\n\n. . .\n\n::: {.absolute left=\"0%\" top=\"50%\" style=\"font-size:1.5em; padding: 0.5em 1em; background-color: rgba(255, 255, 255, .5); backdrop-filter: blur(5px); box-shadow: 0 0 1rem 0 rgba(0, 0, 0, .5); border-radius: 5px;\"}\n\n::: {.callout-warning appearance=\"simple\"}\n\n## Confusing!\n\nThe paper uses the word \"reproducibility\", but in fact they replicated studies. \n\nThat is, they collected new data.\n\n:::\n\n:::\n\n---\n\n## What has gone wrong? Is science a big fraud?\n\n<br>\n\nNo! Fraud in science does happen, but it's rare. \n\n<br>\n\nThere are two main issues: \n\n1. P-hacking\n\n2. File-drawer effect\n\n---\n\n## Your turn: P-hacking\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n\nImagine you had run a fictional experiment (download the data here: [{{< fa table >}} `imaginary_beatle_experiment.csv`](../data/imaginary_beatle_experiment.csv) or from this week's content page) to test whether listening to certain music affects people's behavior or how they perceive themselves. In this fictional experiment, participants were randomly assigned to either:\n\n- **Treatment group**: Listened to \"When I'm Sixty-Four\" by The Beatles.\n\n- **Control group**: Listened to an instrumental jazz piece of similar length.\n\nAfterwards, participants completed a battery of self-report measures and cognitive tasks. \n\nExplore the dataset and look for any evidence that listening to \"When I‚Äôm 64\" had a significant effect on participants (tip, run regressions, you can even add covariates, look only at a subgroup of the data etc.)\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"countdown\" id=\"timer_06465327\" data-update-every=\"1\" data-start-immediately=\"true\" tabindex=\"0\" style=\"right:0;bottom:0;font-size:1em;\">\n<div class=\"countdown-controls\"><button class=\"countdown-bump-down\">&minus;</button><button class=\"countdown-bump-up\">&plus;</button></div>\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">10</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n\n:::\n:::\n\n\n\n---\n\n## Solution \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# You could have tried for example to run this\nmodel <- lm(actual_age ~ condition, data = imaginary_beatle_experiment)\n\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = actual_age ~ condition, data = imaginary_beatle_experiment)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-9.675 -3.043  0.325  2.325  9.957 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       25.6750     0.4665   55.04   <2e-16 ***\nconditionControl  -1.6321     0.6829   -2.39   0.0181 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.172 on 148 degrees of freedom\nMultiple R-squared:  0.03717,\tAdjusted R-squared:  0.03066 \nF-statistic: 5.713 on 1 and 148 DF,  p-value: 0.0181\n```\n\n\n:::\n:::\n\n\n\nA significant difference! But completely implausible. \n\n---\n\n## Solution \n\nThis is how the data was generated:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# a function to simulate the data\nsimulate_phacking_data <- function(n = 150) {\n  tibble(\n    id = 1:n,\n    # simulate the experimental manipulation\n    condition = sample(c(\"Beatles\", \"Control\"), n, replace = TRUE),\n    # all variables below are generate completely independently of the manipulation\n    actual_age = round(rnorm(n, mean = 25, sd = 4)),\n    gender = sample(c(\"Male\", \"Female\", \"Other\"), n, replace = TRUE, prob = c(0.45, 0.45, 0.1)),\n    political_orientation = sample(1:7, n, replace = TRUE),\n    height_cm = round(rnorm(n, mean = 170, sd = 10)),\n    reaction_time_ms = round(rlnorm(n, meanlog = 6.5, sdlog = 0.3)),\n    mood_score = round(rnorm(n, mean = 5, sd = 1.5), 1),\n    memory_score = round(rnorm(n, mean = 10, sd = 3)),\n    confidence = sample(1:7, n, replace = TRUE),\n    perceived_age = actual_age + rnorm(n, mean = 0, sd = 2),\n    stress_level = round(rnorm(n, mean = 4, sd = 1.5), 1),\n    sleep_hours = round(rnorm(n, mean = 7, sd = 1.2), 1),\n    caffeine_intake_mg = round(rnorm(n, mean = 200, sd = 75)),\n    exercise_minutes = round(rnorm(n, mean = 30, sd = 15)),\n    social_media_minutes = round(rnorm(n, mean = 90, sd = 40)),\n    screen_time_hours = round(rnorm(n, mean = 5, sd = 2), 1)\n  )\n}\n```\n:::\n\n\n\n**No true effect** on any other variable. \n\n---\n\n## Solution \n\nAnd this is how the sample was picked:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# simulate until a data set returns a treatment \n# effect on actual age of participants\n\nset.seed(13487)\n\n# Simulate multiple datasets and \"p-hack\" until p < .05\nfind_false_positive <- function(n = 10, max_tries = 10000) {\n  for (i in 1:max_tries) {\n    data <- simulate_phacking_data(n)\n    model <- lm(actual_age ~ condition, data = data)\n    p_val <- tidy(model) |> \n      filter(term == \"conditionControl\") |> \n      pull(p.value)\n    \n    if (p_val < 0.05) {\n      cat(\"üîé Found significant p =\", round(p_val, 4), \"on try\", i, \"\\n\")\n      return(data)\n    }\n  }\n  stop(\"‚ùå No significant result found within max_tries.\")\n}\n\n# Example: Find a \"lucky\" false positive dataset\nimaginary_beatley_experiment <- find_false_positive(n = 150)\n```\n:::\n\n\n\n**Re-sample** until--by chance--a significant result pops up. \n\n---\n\n## P-Hacking\n\n<br>\n\nRemember a **p-value**: \n\nIt's the probability of observing an estimate at least as extreme as the one in our sample, in a world where there is no true effect (the Null world). \n\n<br>\n\nRemember **statistical significance**: \n\nIf p-value < 5% (arbitrary threshold) \n\nWe accept a 5% chance that our results could have occurred in a Null world.\n\n---\n\n## P-Hacking\n\nThat means: \n\nIn a world where there is **no effect**, in 5% of (or 1 out of 20) samples, we find a false positive.\n\n. . .\n\nIf we look at a different outcome variable, that's basically taking a new sample.\n\n. . . \n\nIf we measure 20 outcomes, in a world where there is no effect, we would expect 1 to yield a statistically significant effect.\n\n---\n\n![](images/simmons_false-positive.png){fig-align=\"center\" width=\"100%\"}\n\n---\n\n![](images/simmons_false-positive.png){.absolute top=-50 right=-100 width=\"60%\"}\n\n\n<br>\n<br>\n<br>\n<br>\n<br>\n<br>\n<br>\n\n- To Illustrate how easy p-hacking is, the authors ran an RCT.\n\n- They found that listening to the classic Beatles song ‚ÄúWhen I‚Äôm Sixty-Four‚Äù makes people younger\n\n(Yes, it literally reversed time)\n\n- All analysis were using sound statistical methods and randomization\n\n---\n\n##\n\n![](images/simmons_false-positive.png){.absolute top=-50 right=-100 width=\"60%\"}\n\n\n<br>\n<br>\n<br>\n<br>\n<br>\n\nHow did they achieve it? \n\n. . .\n\n<br>\n\n- **Just like you in the exercise: p-hacking**\n- They collected information about a number of characteristics of their study subjects, and then controlled for one that happened to give them the result they were looking at. (The age of the subject‚Äôs father).\n- They also continued the experiment until they got a significant result, rather than predetermining the sample size.\n\n---\n\n## File-drawer effect\n\n\n![Illustration from [Calling Bullshit](https://callingbullshit.org/)](images/file_drawer.jpg){fig-align=\"center\" width=\"100%\"}\n\n\n---\n\n##\n\n<br>\n\nResearchers are incentives to publish positive, statistically significant results.\n\nBut this can result faulty images of the evidence. \n\n---\n\n## How bad is the issue?\n\n. . .\n\n![[Ioannidis, J. P. A. (2005). Why Most Published Research Findings Are False. PLoS Medicine.](https://doi.org/10.1371/journal.pmed.0020124)](images/ioannidis.png){fig-align=\"center\" width=\"100%\"}\n\n---\n\n## Let's take a step back \n\n<br>\n\nAssume that we have a \"total Null world\" (no true effect for no study in science)\n\n. . .\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|                                                                 |H‚ÇÄ True (100%)    |\n|:----------------------------------------------------------------|:-----------------|\n|**Significant Finding** (Positive result) Œ± = 5%, 1‚àíŒ≤ = 80%      |False Positive 5% |\n|**Non-Significant Finding** (Negative result) 1‚àíŒ± = 95%, Œ≤ = 20% |True Negative 95% |\n\n\n:::\n:::\n\n\n\n---\n\n## Let's take a step back \n\n<br>\n\nNow assume we live in a \"total True world\" (only true effects in all of science)\n\n. . .\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|                                                                 |H‚ÇÄ True (0%) |H‚ÇÅ True (100%)     |\n|:----------------------------------------------------------------|:------------|:------------------|\n|**Significant Finding** (Positive result) Œ± = 5%, 1‚àíŒ≤ = 80%      |0%           |True Positive 80%  |\n|**Non-Significant Finding** (Negative result) 1‚àíŒ± = 95%, Œ≤ = 20% |0%           |False Negative 20% |\n\n\n:::\n:::\n\n\n\n---\n\n## Now let's assume a world where half of science tests true effects\n\n. . .\n\n<br>\n\nFor illustration, let's assume science consists of n = 200 studies\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|                                                                 |H‚ÇÄ True (50%)                                |H‚ÇÅ True (50%)                               |\n|:----------------------------------------------------------------|:--------------------------------------------|:-------------------------------------------|\n|**Significant Finding** (Positive result) Œ± = 5%, 1‚àíŒ≤ = 80%      |False Positive 5% √ó 50% = 2.5% (5 studies)   |True Positive 80% √ó 50% = 40% (80 studies)  |\n|**Non-Significant Finding** (Negative result) 1‚àíŒ± = 95%, Œ≤ = 20% |True Negative 95% √ó 50% = 47.5% (95 studies) |False Negative 20% √ó 50% = 10% (20 studies) |\n\n\n:::\n:::\n\n\n\n---\n\n## And now let's assume a world where only 5% of science tests true effects\n\n. . .\n\n<br>\n\nIn other words, we assume scientists generally test implausible hypotheses\n\n. . .\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|                                                                              |$H_0$ True (95\\%)                                |$H_1$ True (5\\%)                         |\n|:-----------------------------------------------------------------------------|:------------------------------------------------|:----------------------------------------|\n|**Significant Finding** (Positive result) $\\alpha = 5\\%, 1-\\beta = 80\\%$      |False Positive 5% √ó 95% = 4.75% (9.5 studies)    |True Positive 80% √ó 5% = 4% (8 studies)  |\n|**Non-Significant Finding** (Negative result) $1-\\alpha = 95\\%, \\beta = 20\\%$ |True Negative 95% √ó 95% = 90.25% (180.5 studies) |False Negative 20% √ó 5% = 1% (2 studies) |\n\n\n:::\n:::\n\n\n\n---\n\n## We never know the ratio of true vs. null effects in science...\n\n. . .\n\n<br>\n\n...but what we would like to know is: \n\n<br>\n\nGiven that we observe a statistically significant effect, what's the probability that it is true? \n\n---\n\n## Positive Predictive Value (PPV)\n\n<br>\n\n$$\n\\text{PPV} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}} = \\frac{8}{8 + 9.5} = \\frac{8}{17.5} \\approx 45.7\\%\n$$\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|                                                                              |$H_0$ True (95\\%)                                                                               |$H_1$ True (5\\%)                                                                            |\n|:-----------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------|\n|**Significant Finding** (Positive result) $\\alpha = 5\\%, 1-\\beta = 80\\%$      |<span style='color:red; font-weight:bold;'>False Positive 5% √ó 95% = 4.75% (9.5 studies)</span> |<span style='color:green; font-weight:bold;'>True Positive 80% √ó 5% = 4% (8 studies)</span> |\n|**Non-Significant Finding** (Negative result) $1-\\alpha = 95\\%, \\beta = 20\\%$ |True Negative 95% √ó 95% = 90.25% (180.5 studies)                                                |False Negative 20% √ó 5% = 1% (2 studies)                                                    |\n\n\n:::\n:::\n\n\n\n---\n\n![](images/ioannidis.png){.absolute top=-50 right=-100 width=\"60%\"}\n\n\n\n<br>\n<br>\n<br>\n<br>\n<br>\n\nThis is Ioannidis' argument: \n\n. . .\n\nIf we assume that \n\n- scientists test implausible hypotheses\n\n- and that only a fraction of negative results gets reported\n\nthen most of research findings are wrong (i.e. low PPV).\n\n. . .\n\n::: {.callout-note appearance=\"simple\"}\n\n## This is likely an overestimation of the problem!\n\nOne can argue with Ioannidis' assumptions. \n\nAlso, the paper is old (2005), and many things have changed for the better since.\n\n:::\n\n---\n\n## US Food and Drug Administration (FDA) \n\n:::{.incremental}\n\n- In the United States clinical trials (experiments using human subjects to test outcomes of medical treatments) are required by law to register this trial with the FDA\n\n- This involves explaining what the trial is designed to test, how the trial will be conducted, and how the outcomes will be measured. \n\n- Once the trial is completed, the researchers are also required to report the results to the FDA.\n\n- However, they are not required to publish the results in a scientific journal.\n\n:::\n\n---\n\n![[Turner, E. H., Matthews, A. M., Linardatos, E., Tell, R. A., & Rosenthal, R. (2008). Selective Publication of Antidepressant Trials and Its Influence on Apparent Efficacy. New England Journal of Medicine.](https://doi.org/10.1056/NEJMsa065779)](images/turner_anti-depressant.png){fig-align=\"center\" width=\"100%\"}\n\n---\n\n\n![Illustration from [Bergstrom, C. T., & West, J. D. (2020). Calling bullshit: The art of skepticism in a data-driven world (First edition). Random House.](https://callingbullshit.org/)](images/fda_anti-depressant.png){fig-align=\"center\" width=\"100%\"}\n\n\n# How to do better science?\n\n---\n\n## Two main solutions\n\n1. At the least, make data and code public\n\n(at best, write reproducible manuscripts)\n\n---\n\nThis is something you've learned by using Quarto! üéâ \n\n![](images/quarto-reproducible-illustration.png){fig-align=\"center\" width=\"100%\"}\n\n---\n\n## Two main solutions\n\n1. At the least, make data and code public\n\n(at best, write reproducible manuscripts)\n\n. . .\n\n2. Preregister studies\n\n---\n\n![](images/guardian-replication-revolution.png){.absolute left=0 bottom=0 width=\"80%\"}\n\n![](images/guardian-logo.png){.absolute top=0 right=0 width=\"30%\"}\n\n--- \n\n## Pre-registration in a nutshell\n\nA time-stamped document describing the details of the planned study\n\n. . .\n\nBenefits:\n\n- makes research process more transparent \n\n- allows to clearly distinguish a priori and post-hoc decisions\n\n- makes it harder to fool (the researchers themselve and others)\n\n---\n\n## How exactly does it work? \n\n![](images/osf-logo.png){.absolute top=-50 right=-100 width=\"40%\"}\n\n\n\n<br>\n<br>\n<br>\n<br>\n<br>\n\nThere are different options but the most common one is to use the [Open Science Framework (OSF)](https://osf.io)\n\n- large number of templates\n\n- all participating authors get informed and can cancel within 48h\n\n- prereg can be kept privat for a while (if needed to protect the project)\n\n---\n\n## What goes into a preregistration?\n\nAnything that might be considered a researcher degree of freedom\n\n- fixed decisions (e.g. the research design, outcome measures)\n\n- decision rules (e.g. \"if we cannot collect 100 participants until June 5 2025, we will stop the data collection that very day and...\")\n\n- statistical models (ideally analysis scripts, including data-dependent decision rules)\n\n- sample size and power analysis\n\n---\n\n![From [Dattani, S. (2022). Why randomized controlled trials matter and the procedures that strengthen them. Our World in Data. ](https://ourworldindata.org/randomized-controlled-trials)](images/fda_registration.jpg){fig-align=\"center\" width=\"100%\"}\n\n---\n\n## Two main solutions\n\n1. At the least, make data and code public\n\n(at best, write reproducible manuscripts)\n\n2. Preregister studies\n\n. . .\n\n3. Change the scientific publishing system\n\n---\n\n## Registered reports\n\n<br>\n\n![[Illustration from the OSF](https://www.cos.io/initiatives/registered-reports)](images/registered_reports.png){fig-align=\"center\" width=\"100%\"}\n\n. . .\n\n- journals commit to publishing results, no matter whether significant or not\n\n- relevance of a study is determined based on the research question, not the result\n\n---\n\n![[Scheel, A. M., Schijen, M. R. M. J., & Lakens, D. (2021). An Excess of Positive Results: Comparing the Standard Psychology Literature With Registered Reports. Advances in Methods and Practices in Psychological Science.](https://doi.org/10.1177/25152459211007467)](images/registered_reports_success.png){fig-align=\"center\" width=\"100%\"}\n\n---\n\nThat's it for today :)\n\n\n\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../site_libs/countdown-0.4.0/countdown.css\" rel=\"stylesheet\" />\n<script src=\"../site_libs/countdown-0.4.0/countdown.js\"></script>\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}