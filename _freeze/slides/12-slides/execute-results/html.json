{
  "hash": "6a9891ce1809c16e5c3a43c252a8562b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Meta Analyses\"\nformat: \n  revealjs:\n    theme: simple\n    smaller: true\n    slide-number: true\n    # incremental: true\n    # code-link: true\n    chalkboard: true\n    history: false\n---\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n\n\n## Overview\n\n::::: columns\n::: {.column style=\"font-size: smaller;\"}\n1.  **Why meta-analyses**\n\n2.  **The research question**\n\n3.  **Effect Sizes**\n\n4.  **Systematic literature search**\n\n5.  **Running a meta-analysis**\n:::\n\n::: {.column style=\"font-size: smaller;\"}\n\n:::\n:::::\n\n# Why meta-analyses?\n\n---\n\n## (A typical textbook) pyramid of evidence\n\n![](images/pyramid_of_evidence.png){.center width=\"100%\"}\n\n---\n\n## Why are meta-analyses at the top of the pyramid?\n\n-   Results of single studies are affected by many factors:\n\n    -   the country\n    -   experimental setup\n    -   researchers\n    -   sample demographics...\n\n-   Averaging across many studies, we get a more robust, generalizable result\n\n# The research question\n\n---\n\n## Case study: “Can people tell true news from false News ?”\n\n>Pfänder, J., & Altay, S. (2025). Spotting false news and doubting true news: A systematic review and meta-analysis of news judgements. Nature Human Behaviour, 1–12. [https://doi.org/10.1038/s41562-024-02086-1](https://doi.org/10.1038/s41562-024-02086-1)\n\n---\n\n## Case study: “Can people tell true news from false News ?”\n\n::::: columns\n::: {.column width=\"50%\"}\n\n![[Pennycook, G., Binnendyk, J., Newton, C., & Rand, D. G. (2021). A Practical Guide to Doing Behavioral Research on Fake News and Misinformation. Collabra: Psychology, 7(1), 25293.](https://doi.org/10.1525/collabra.25293)](images/example-item.jpg){fig-align=\"center\" width=\"100%\"}\n\n:::\n\n::: {.column width=\"50%\"}\n![](images/example-item2.jpg){width=\"100%\"}\n:::\n:::::\n\n---\n\n## Define a quantifiable outcome\n\n<br>\n\n<br>\n\n$$\n\\text{discernment} = \\text{mean accuracy}_{\\text{true news}} - \\text{mean accuracy}_{\\text{false news}}\n$$\n\n---\n\n## Your turn: \n\n\n1. Download the data from:\n\n>Lyons, B., Modirrousta-Galian, A., Altay, S., & Salovich, N. A. (2024). Reduce blind spots to improve news discernment? Performance feedback reduces overconfidence but does not improve subsequent discernment. https://doi.org/10.31219/osf.io/kgfrb \n\n[{{< fa table >}} `Lyons_2024.csv`](../data/Lyons_2024.csv)\n\n2. Calculate a discernment score\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"countdown\" id=\"timer_e0c9c48d\" data-update-every=\"1\" data-start-immediately=\"true\" tabindex=\"0\" style=\"right:0;bottom:0;font-size:1em;\">\n<div class=\"countdown-controls\"><button class=\"countdown-bump-down\">&minus;</button><button class=\"countdown-bump-up\">&plus;</button></div>\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">05</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n\n:::\n:::\n\n\n\n---\n\n## Solution {visibility=\"hidden\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlyons |> \n  group_by(veracity) |> \n  summarize(mean = mean(accuracy, na.rm = TRUE)) |> \n  pivot_wider(names_from = veracity, \n              values_from = mean) |> \n  mutate(discernment = true - fake)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n   fake  true discernment\n  <dbl> <dbl>       <dbl>\n1  1.86  2.56       0.695\n```\n\n\n:::\n:::\n\n\n\n. . .\n\nIs this score large? \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(lyons$scale)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n    4 \n10671 \n```\n\n\n:::\n:::\n\n\n\nThe study used a 4-point scale. On this scale, a value of 0.69 seems quite large. \n\n\n# Effect sizes\n\n---\n\n## What are effect sizes? \n\nIn Meta-analyses not individual participants, but single studies are the unit of analysis\n\nThe outcomes of these studies are called \"effect sizes\".\n\n![Graphic adapted from: [Harrer, M., Cuijpers, P., A, F. T., & Ebert, D. D. (2021). Doing Meta-Analysis With R: A Hands-On Guide (1st ed.). Chapman & Hall/CRC Press.](https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/)](images/meta_level_analysis.png){fig-align=\"center\" width=\"100%\"}\n\n---\n\n## Your turn: \n\n\n1. Download the data from\n\n>Allen, J., Arechar, A. A., Pennycook, G., & Rand, D. G. (2021). Scaling up fact-checking using the wisdom of crowds. Science Advances, 7(36), eabf4393.\n\n[{{< fa table >}} `Allen_2021.csv`](../data/Allen_2021.csv)\n\n2. Calculate the discernment score.\n\n3. How does this compare to the other study? What issue are you seeing when you want to compare the two?\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"countdown\" id=\"timer_ba32071a\" data-update-every=\"1\" data-start-immediately=\"true\" tabindex=\"0\" style=\"right:0;bottom:0;font-size:1em;\">\n<div class=\"countdown-controls\"><button class=\"countdown-bump-down\">&minus;</button><button class=\"countdown-bump-up\">&plus;</button></div>\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">05</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n\n:::\n:::\n\n\n\n---\n\n## Solution {visibility=\"hidden\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nallen |> \n  group_by(veracity) |> \n  summarize(mean = mean(accuracy, na.rm = TRUE)) |> \n  pivot_wider(names_from = veracity, \n              values_from = mean) |> \n  mutate(discernment = true - fake)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n   fake  true discernment\n  <dbl> <dbl>       <dbl>\n1  3.82  4.74       0.915\n```\n\n\n:::\n:::\n\n\n\nCheck the scale\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nunique(allen$scale)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"7\"\n```\n\n\n:::\n:::\n\n\n\nThe effect size is bigger than the one we found for Lyons 2024, but it is also measured on a broader 7-point scale.\n\nThis makes it impossible to immediately compare the effect sizes.\n\n---\n\n## Non-standardized effect sizes\n\n<br>\n\n...can only be directly interpreted on their original scales\n\n<br>\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|paper_id     |scale | fake| true| discernment|\n|:------------|:-----|----:|----:|-----------:|\n|Allen_2021   |7     | 3.82| 4.74|        0.92|\n|Lyons_2024_b |4     | 1.86| 2.56|        0.69|\n\n\n:::\n:::\n\n\n\n<br>\n\nThis is not great if we want to compare studies.\n\n---\n\n## Standardized effect sizes\n\n<br>\n\n... are not dependent on specific scales\n\n<br>\n\nThey allow us to compare effects across studies that use different outcome measures.\n\n---\n\n## Cohen's d\n\n<br>\n\n- Perhaps the most popular standardized effect size when comparing two groups on a continuous outcome\n\n<br>\n\n- The idea is to express effect sizes in units of standard deviations\n\n---\n\n## Cohen's d\n\n- If we plot the distribution of all individual accuracy ratings, we get something like below (slightly less perfect). \n\n- Cohen's d uses the standard devations of these distributions\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](12-slides_files/figure-revealjs/unnamed-chunk-10-1.png){width=960}\n:::\n:::\n\n\n\n---\n\n## Cohen's d \n\n<br>\n\n$$\n\\text{Cohen's d} = \\frac{\\bar{x}_{\\text{true}} - \\bar{x}_{\\text{false}}}{SD_{\\text{pooled}}}\n$$ \n\nwith\n\n$$\nSD_{\\text{pooled}} = \\sqrt{\\frac{SD_{\\text{true}}^2+SD_{\\text{false}}^2}{2}}\n$$\n\n---\n\n## Your turn: \n\nIn which of the two studies did participants discern better? Calcuate Cohens' d for the two studies and compare. \n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"countdown\" id=\"timer_f915bb78\" data-update-every=\"1\" data-start-immediately=\"true\" tabindex=\"0\" style=\"right:0;bottom:0;font-size:1em;\">\n<div class=\"countdown-controls\"><button class=\"countdown-bump-down\">&minus;</button><button class=\"countdown-bump-up\">&plus;</button></div>\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">05</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n\n:::\n:::\n\n\n\n---\n\n## Solution {visibility=\"hidden\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npooled_sd <- function(sd_true, sd_false) {\n  sd_pooled <- sqrt((sd_true^2 + sd_false^2) / 2)\n  return(sd_pooled)\n}\n\n# Cohen's d for Allen_2021\nallen_cohen_d <- allen |> \n  group_by(veracity) |> \n  summarize(mean = mean(accuracy, na.rm = TRUE), \n            sd = sd(accuracy, na.rm = TRUE)) |> \n  pivot_wider(names_from = veracity, \n              values_from = c(mean, sd)) |> \n  mutate(discernment = mean_true - mean_fake, \n         # calculate Cohen's d\n         pooled_sd = pooled_sd(sd_true, sd_fake), \n         cohen_d = discernment/pooled_sd\n         )\n\n# Cohen's d for Lyons_2024\nlyons_cohen_d <- lyons |> \n  group_by(veracity) |> \n  summarize(mean = mean(accuracy, na.rm = TRUE), \n            sd = sd(accuracy, na.rm = TRUE)) |> \n  pivot_wider(names_from = veracity, \n              values_from = c(mean, sd)) |> \n  mutate(discernment = mean_true - mean_fake, \n         # calculate Cohen's d\n         pooled_sd = pooled_sd(sd_true, sd_fake), \n         cohen_d = discernment/pooled_sd\n         )\n\n# compare\nlyons_cohen_d$cohen_d - allen_cohen_d$cohen_d \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2769006\n```\n\n\n:::\n:::\n\n\n\n---\n\n## Interpreting Cohen's d\n\n\"Participants in Lyons (2024) discerned better between true and false news than participants in Allen (2021), by 0.28 standard deviations\"\n\n. . .\n\nAlthough widely used, this is not always easy to interpret. In psychology, as a rough guide for intepreting Cohen's d:\n\n- small (0.2) < medium (0.5) < large (0.8)\n\n(**but**: depends a lot on the discipline, research question etc.)\n\n. . .\n\nIf you want to play around with the interpretation of Cohen's d, [check out this guide by Kristoffer Magnusson](https://rpsychologist.com/cohend/)\n\n\n# Systematic literature review\n\n---\n\n::: {.callout-tip}\n\n- Literature search something you do before calculating effect sizes, of course.\n\n- But to do an effective literature search, you need to have an idea of what you are looking for.\n\n- So, it makes sense to start thinking about the effect size you want to calculate before you do the literature search.\n\n- In general, we will only very superficially talk about the literature search here.\n\n:::\n\n---\n\n## The search string\n\nIdeally, we want all studies that have ever been written on our research question. The more the better.\n\n. . . \n\nBut...\n\n![Screenshot from a google scholar search on June 27, 2024](images/scholar.png){width=\"70%\"}\n\nWe often need to be specific in our search. \n\n---\n\n## The search string\n\n<br>\n\n- When you start a meta-analysis, you often have at least a rough idea of what you are looking for/ have a paper that inspired your idea\n\n- This gives you ideas of keywords that you could look for.\n\n---\n\n## The search string\n\nThis was our search string:\n\n> '\"false news\" OR \"fake news\" OR \"false stor\\*\" AND \"accuracy\" OR \"discernment\" OR \"credibilit\\*\" OR \"belief\" OR \"susceptib\\*\"'\n\n. . .\n\n:::: quiz\n<i class=\"fa fa-question-circle quiz-icon\"></i>\n\n<div>\n\nWould this search string have yielded a study called \"News accuracy ratings of US adults during 2016 presidential elections\"?\n\n</div>\n::::\n\n. . . \n\nNo, because of how boolean operators work. \n\n---\n\n## Boolean operators\n\n::::: columns\n::: {.column width=\"50%\"}\n![Boolean AND operator. Only if both keywords are included, a result will show up.](images/boolean_operator_AND.png){width=\"100%\"}\n:::\n\n::: {.column width=\"50%\"}\n![Boolean OR operator. As long as one of the keywords is included, a result will show up.](images/boolean_operator_OR.png){width=\"100%\"}\n:::\n:::::\n\n---\n\n## Boolean operators\n\n> '\"false news\" OR \"fake news\" OR \"false stor\\*\" AND \"accuracy\" OR \"discernment\" OR \"credibilit\\*\" OR \"belief\" OR \"susceptib\\*\"'\n\n<br>\n\n- Our search string, put a bit more abstractly, reads ...OR...OR...AND...Or...OR... \n\n- As in math, there is a hierarchy among operators. \n\n- On Scopus (the search engine we used), [OR operators are treated before AND operators](https://schema.elsevier.com/dtds/document/bkapi/search/SCOPUSSearchTips.htm).\n\n---\n\n## Data bases\n\n- Google Scholar -- great in most cases, but some disadvantages (e.g. user-specific results)\n\n<br>\n\n- There are many other data bases out there: \n\n  - Pubmed\n  - Scopus\n  - Web of Science\n  - Your local university library catalogue\n  - ...\n\n---\n\n## Data bases\n\nData bases allow very refined searches. \n\n::::: columns\n::: {.column width=\"50%\"}\n![](images/scopus_1.png){width=\"100%\"}\n:::\n\n::: {.column width=\"50%\"}\n![](images/scopus_2.png){width=\"100%\"}\n:::\n:::::\n\n---\n\n## Data bases\n\nSome databases also have features to export your search results as data sets. \n\n![](images/scopus_6.png){width=\"50%\"}\n\n\n---\n\n## Literature screening\n\nThere are several stages of deciding which studies to include:\n\n1. Title screening (sort out titles that are obviously irrelevant)\n2. Abstract screening \n3. Full-text screening\n\n. . .\n\nIn particular in screening phases 2 and 3, all decisions are based on inclusion criteria (and must be documented!). \n\nFor example, some of our inclusion criteria were: \n\n- english language\n- a measure of accuracy for both true and false news\n- real-world news items only\n\n---\n\n## PRISMA guidelines\n\n<br>\n\n- The whole point of a systematic search is to have an exhaustive and unbiased pool of studies. \n\n- We won't discuss the whole process here, but if you ever do a systematic review, you'll want to check out the [PRISMA guidelines](https://www.prisma-statement.org/) for systematic reviews and meta-analyses\n\n\n# Running a meta-analysis\n\n---\n\n## The meta-analytic average\n\n- A meta-analysis is basically taking an average of all effect sizes\n\n![Graphic adapted from: [Harrer, M., Cuijpers, P., A, F. T., & Ebert, D. D. (2021). Doing Meta-Analysis With R: A Hands-On Guide (1st ed.). Chapman & Hall/CRC Press.](https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/)](images/meta_level_analysis.png){fig-align=\"center\" width=\"100%\"}\n\n---\n\n## The meta-analytic average\n\n<br>\n\n...but not just a normal average - a weighted average. \n\n- The idea is that larger studies (with more participants/observations) are given more weight than smaller studies\n\n- This is typically done via the standard error of the effect sizes\n\n---\n\n## Cohen's d (*part II*): the standard error\n\n::: {.callout-tip}\nRemember: \n\nThe standard error is a special standard deviation - the standard deviation of the sampling distribution (i.e. the--hypothetical--distribution of estimates we would expect from drawing many different samples from a population). \n:::\n\n---\n\n## Cohen's d (*part II*): the standard error\n\n<br>\n\n$$\nSE_{\\text{Cohen's d}} = \\sqrt{ \\frac{n_\\text{true} + n_\\text{false}}{n_\\text{true} n_\\text{false}} + \\frac{d^2}{2(n_\\text{true} + n_\\text{false})} }\n$$\n\n- $d$ = Cohen's d\n- $n_\\text{false}$ =  sample size of fake news items, \n- $n_\\text{true}$ = sample size of true news items, \n- and $SD_{\\text{pooled}}$ the **pooled standard deviation** of both groups (see above).\n\n. . .\n\n::: {.callout-tip}\nAll you need to remember: \n\nWith greater sample size, smaller standard error (this should ring a bell from our class on statistical power).\n:::\n\n---\n\nYour turn: \n\nImagine you collected data from 9 studies\n\n1. Download the meta-analysis data [{{< fa table >}} `meta_analysis_news.csv`](../data/meta_analysis_news.csv).\n\n2. Calculate Cohen's d and its standard error (you can use this function which translates the previous formula into R code):\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"countdown\" id=\"timer_51156d47\" data-update-every=\"1\" data-start-immediately=\"true\" tabindex=\"0\" style=\"right:0;bottom:0;font-size:1em;\">\n<div class=\"countdown-controls\"><button class=\"countdown-bump-down\">&minus;</button><button class=\"countdown-bump-up\">&plus;</button></div>\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">05</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n\n:::\n:::\n\n\n\n---\n\n## Solution {visibility=\"hidden\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Function to calculate pooled standard deviation\npooled_sd <- function(sd_true, sd_false) {\n  sd_pooled <- sqrt((sd_true^2 + sd_false^2) / 2)\n  return(sd_pooled)\n}\n\n# Function to calculate standard error of Cohen's d\nse_cohen_d <- function(d, n1, n2) {\n  se_d <- sqrt((n1 + n2) / (n1 * n2) + (d^2) / (2 * (n1 + n2)))\n  return(se_d)\n}\n\nmeta_summary <- meta_summary |> \n  mutate(pooled_sd = pooled_sd(sd_true, sd_fake), \n         cohen_d = discernment/pooled_sd, \n         se_cohen_d = se_cohen_d(cohen_d, n_ratings_true, n_ratings_fake)\n         )\n```\n:::\n\n\n\n---\n\n## Calculate weigths \n\n<br>\n\nAs noted earlier, the standard error helps us calculate a weight for each study $k$ in our meta-analysis: \n\n$$\nw_k = \\frac{1}{se^2_k}\n$$\n\n- $se^2$ = squared standard error (also called variance) of a study $k$\n\n::: {.callout-tip}\nNote: \n\nThe bigger the sample size, the smaller the standard error, the greater the weight. \n:::\n\n---\n\n## Calculate the meta-analytic average\n\n<br>\n\nBased on the standardized effect sizes and their standard errors, we can finally calculate the meta-analytic average.\n\n$$\n\\hat\\theta = \\frac{\\sum^{K}_{k=1} \\hat\\theta_kw_k}{\\sum^{K}_{k=1} w_k}\n$$\n\n- $\\hat\\theta$ = estimate of the meta-analytic average\n\n---\n\n## The meta-analytic average\n\n<br>\n\nIn practice, you'll always use existing R packages for these calculations. \n\nThere are many packages out there, but a popular one is the [`metafor`](https://www.metafor-project.org/doku.php/metafor) package\n\n---\n\n## Your turn: \n\nCan people discern between true and false news? Use the metafor package and its [`rma.uni()`](https://wviechtb.github.io/metafor/reference/rma.uni.html) function (check documentation by clicking on the function) to calculate the meta-analytic average. \n\n::: {.callout-tip}\n\n- you don't need to specify the `weights` arguments\n\n- you only need to specify `yi`, `sei` and `data`\n\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"countdown\" id=\"timer_fa0c2b3a\" data-update-every=\"1\" data-start-immediately=\"true\" tabindex=\"0\" style=\"right:0;bottom:0;font-size:1em;\">\n<div class=\"countdown-controls\"><button class=\"countdown-bump-down\">&minus;</button><button class=\"countdown-bump-up\">&plus;</button></div>\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">05</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n\n:::\n:::\n\n\n\n---\n\n## Solution {visibility=\"hidden\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(metafor)\n\nmeta_model <- rma.uni(yi = cohen_d, sei = se_cohen_d, data = meta_summary)\n\nsummary(meta_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nRandom-Effects Model (k = 8; tau^2 estimator: REML)\n\n  logLik  deviance       AIC       BIC      AICc   \n -0.8852    1.7704    5.7704    5.6622    8.7704   \n\ntau^2 (estimated amount of total heterogeneity): 0.0748 (SE = 0.0402)\ntau (square root of estimated tau^2 value):      0.2735\nI^2 (total heterogeneity / total variability):   99.79%\nH^2 (total variability / sampling variability):  475.36\n\nTest for Heterogeneity:\nQ(df = 7) = 3115.1291, p-val < .0001\n\nModel Results:\n\nestimate      se    zval    pval   ci.lb   ci.ub      \n  0.8318  0.0970  8.5780  <.0001  0.6418  1.0219  *** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n---\n\n## Forest plot\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Points are effect sizes, bars are 95% confidence intervals - an indicator of statistical significance we haven't discussed yet. The idea is that if they exclude 0, that's like having a p-value below 0.05: the estimate is statistically significant.](12-slides_files/figure-revealjs/unnamed-chunk-18-1.png){width=960}\n:::\n:::\n\n\n\n\n# Things we couldn't cover (but you should know are possible)\n\n---\n\n- There are many other standardized effect sizes for different variably types \n\n- We have only discussed fixed effects meta-analyses. Yet, for some research questions and data types, it is better to use slightly more complex, random effects meta-analyses \n\n- To test the effect of context factors, you can also run so called \"meta-regressions\"\n\n- Meta-analyses can reveal publication bias or p-hacking (see our class the replication crisis). Meta-analyses typically include several tests that can be indicative of this \n\n",
    "supporting": [
      "12-slides_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../site_libs/countdown-0.4.0/countdown.css\" rel=\"stylesheet\" />\n<script src=\"../site_libs/countdown-0.4.0/countdown.js\"></script>\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}