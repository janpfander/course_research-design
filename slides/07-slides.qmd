---
title: "Linear Regression"
format: 
  revealjs:
    theme: simple
    smaller: true
    slide-number: true
    # incremental: true
    # code-link: true
    chalkboard: true
    history: false
---

```{r}
library(tidyverse)
library(kableExtra)
library(truncnorm)
library(broom)
```

## Overview

::::: columns
::: {.column style="font-size: smaller;"}
1.  **Drawing Lines**

2. **Running a regression in R**

3.  **Statistical Inference for Regression**

:::

::: {.column style="font-size: smaller;"}

:::
:::::

# Drawing Lines

---

## Essential parts of regression 

::::: columns
::: {.column}
**Y**

<br>

"Outcome"/"Dependent"/"Response" variable

<br>

Thing you want to explain or predict

:::

::: {.column}
**X**

<br>

"Explanatory"/"Independent"/"Predictor" variable

<br>

Thing you use to explain or predict Y
:::

:::::

---

## For example, **Does eating cookies make people happier?**

---

## Cookies and happiness

```{r make-cookies}
#| include: false
cookies <- tibble(happiness = c(0.5, 2, 1, 2.5, 3, 1.5, 2, 2.5, 2, 3),
                  cookies = 1:10)

# alternative of getting predictions with broom package
#cookies_model <- lm(happiness ~ cookies, data = cookies)
# cookies_fitted <- augment(cookies_model)

# Compute the OLS regression model
ols_model <- lm(happiness ~ cookies, data = cookies)
cookies_fitted <- augment(ols_model)

# Define incorrect lines
bad_slope <- function(x) 0.1 * x + 1  # Too flat
bad_intercept <- function(x) 0.4 * x -1  # Too high

cookies_fitted <- cookies_fitted %>%
  mutate(pred_bad_slope = bad_slope(cookies),
         pred_bad_intercept = bad_intercept(cookies))

sum_squared_residuals <- function(data, pred_col) {
  sum((data$happiness - data[[pred_col]])^2)
}

sum_res_bad_slope <- sum_squared_residuals(cookies_fitted, "pred_bad_slope")
sum_res_bad_intercept <- sum_squared_residuals(cookies_fitted, "pred_bad_intercept")
sum_res_ols <- sum_squared_residuals(cookies_fitted, ".fitted")
```

```{r cookies-base, echo=FALSE, message=FALSE, fig.dim=c(8, 4), out.width="100%"}
cookies_base <- ggplot(cookies_fitted, aes(x = cookies, y = happiness)) +
  geom_point(size = 3) +
  scale_x_continuous(breaks = 0:10) +
  labs(x = "Cookies eaten", y = "Level of happiness") +
  theme_minimal(base_size = 14) +
  theme(panel.grid.minor = element_blank(),
        plot.title = element_text(face = "bold"))

cookies_base
```

---

### How to do regression?

1. Plot X and Y

<br>

2. Draw a line that approximates the relationship

<br>

3. Find out the slope and intercept of the line

---

## How good is the fit? 

```{r cookies-bad-slope, echo=FALSE, message=FALSE, fig.dim=c(8, 4), out.width="100%"}
# 1. Bad slope
cookies_base +
  geom_abline(intercept = 1, slope = 0.1, color = "red", linetype = "dashed") 
```

---

## How good is the fit? 

```{r cookies-bad-intercept, echo=FALSE, message=FALSE, fig.dim=c(8, 4), out.width="100%"}
# 2. Bad intercept
cookies_base +
  geom_abline(intercept = -1, slope = 0.4, color = "red", linetype = "dashed") +
  coord_cartesian(xlim = c(1, 10), ylim = c(-1, 3)) 
```

---

## How good is the fit? 

```{r cookies-lm, echo=FALSE, message=FALSE, fig.dim=c(8, 4), out.width="100%"}
cookies_base +
  geom_smooth(method = "lm", color = "#0074D9", se = FALSE, linetype = "dashed")
```

---

## Residuals

We don't need to rely on our visual intuition. We can calculate how well the line fits our data. 

. . .

<br>

**Residual = difference between the observed and predicted values of the dependent variable**

(i.e. distance of a data point to the line, for a given value of X)

. . .

<br> 

For example, if someone who ate 5 cookies reported a happiness level of 2.5, but the regression predicts 2.0, the residual is:

$2.5âˆ’2.0=0.5$

---

## Ordinary Least Squares (OLS)

<br>

Goal of regression: minimize residuals

. . . 

A (not so good solution): Take the sum of all residuals

  - some residuals are negative, some are positive, they will cancel out
  
. . .
  
A better solution: Square all residuals, then sum them

  - this is a strategy we've already seen for calculating standard deviations. 
  
. . .

::: {.callout-tip}
## Why not just use absolute values?

It is mathematically a lot easier to find the minimum of squared sums. 
:::

---

```{r plot-with-residuals}
#| include: false
# Function to generate plots with residuals
plot_with_residuals <- function(data, pred_col, title = NULL, sum_squared_residuals, line_color) {
  p <- ggplot(data, aes(x = cookies, y = happiness)) +
    geom_point(size = 3) +
    geom_segment(aes(xend = cookies, yend = !!sym(pred_col)), 
                 color = "gray50", linetype = "dotted") +  # Residuals
    geom_line(aes(y = !!sym(pred_col)), color = line_color, linetype = "dashed") + # Regression line
    scale_x_continuous(breaks = 0:10) +
    labs(x = "Cookies eaten", y = "Level of happiness") +
    theme_minimal(base_size = 14) +
    theme(panel.grid.minor = element_blank())

  # Add title if not NULL
  if (!is.null(title)) {
    p <- p + labs(title = paste0(title, "\nSquared Sum of Residuals: ", round(sum_squared_residuals, 2))) +
      theme(plot.title = element_text(face = "bold"))
  }
  
  return(p)
}

```

```{r cookies-bad-slope-residuals, echo=FALSE, message=FALSE, fig.dim=c(8, 4), out.width="100%"}
plot_with_residuals(cookies_fitted, "pred_bad_slope", "Line 1", sum_res_bad_slope, "red")
```

---

```{r cookies-bad-intercept-residuals, echo=FALSE, message=FALSE, fig.dim=c(8, 4), out.width="100%"}
plot_with_residuals(cookies_fitted, "pred_bad_intercept", "Line 2", sum_res_bad_intercept, "red") +
  coord_cartesian(xlim = c(1, 10), ylim = c(-1, 3)) 
```

---

```{r cookies-lm-residuals, echo=FALSE, message=FALSE, fig.dim=c(8, 4), out.width="100%"}
plot_with_residuals(cookies_fitted, ".fitted", "Line 3 (Best Fit)", sum_res_ols, "#0074D9")
```

---

## Linear regression estimates

```{r highlight-slope, echo=FALSE, message=FALSE, fig.dim=c(8, 4), out.width="100%"}
# Fit the linear model
lm_fit <- lm(happiness ~ cookies, data = cookies)

# Extract the intercept and slope
intercept <- coef(lm_fit)[1]
slope <- coef(lm_fit)[2]

# Create the plot with the highlighted slope
cookies_base +
  geom_smooth(method = "lm", color = "#0074D9", se = FALSE) 
```

---

## Linear regression estimates

```{r baseline-regression, echo=FALSE, message=FALSE, fig.dim=c(8, 4), out.width="100%"}
# Fit the linear model
lm_fit <- lm(happiness ~ cookies, data = cookies)

# Extract the intercept and slope
intercept <- coef(lm_fit)[1]
slope <- coef(lm_fit)[2]

# Create the plot with the highlighted slope
slope_highlight <- cookies_base +
  geom_smooth(method = "lm", color = "#0074D9", se = FALSE) +
  
  # Draw the right triangle showing the slope
  geom_segment(aes(x = 5, y = intercept + slope * 4, xend = 5, yend = intercept + slope * 5), 
               linetype = "dotted", color = "black") +  # Vertical line
  geom_segment(aes(x = 4, y = intercept + slope * 4, xend = 5, yend = intercept + slope * 4), 
               linetype = "dotted", color = "black") +  # Horizontal line
  
  # Annotate the slope value next to the vertical line
  annotate("text", x = 5.2, y = intercept + slope * 4.5, 
           label = paste0("slope: ", round(slope, 2)), 
           color = "black", size = 5, hjust = 0)

slope_highlight 
```

---

## Linear regression estimates

```{r highlight-slope-intercept, echo=FALSE, message=FALSE, fig.dim=c(8, 4), out.width="100%"}
# Create the plot with the highlighted slope
intercept_slope_highlight <- slope_highlight  +
  # Highlight the intercept with a vertical dotted line
  geom_segment(aes(x = 0, y = 0, xend = 0, yend = intercept), 
               linetype = "dotted", color = "black") +  # Vertical line from x=0 to intercept
  # Annotate the intercept value next to the vertical line
  annotate("text", x = 0.2, y = intercept, 
           label = paste0("intercept: ", round(intercept, 2)), 
           color = "black", size = 5, hjust = 0) +
  # Extend the regression line from the last data point to the y-axis as a dotted line
  geom_segment(aes(x = min(cookies), y = intercept + slope * min(cookies), 
                   xend = 0, yend = intercept), 
               linetype = "dotted", color = "#0074D9")

intercept_slope_highlight 
```

---

## Interpreting linear regression output

::::: columns
::: {.column width="50%"}

"A one unit increase in X is associated with a beta 1 increase (or decrease) in Y, on average."

```{r}
#| echo: false
intercept_slope_highlight 
```

:::

::: {.column width="50%"}

::: {.fragment}
```{r}
#| echo: true
# Compute the OLS regression model
lm(happiness ~ cookies, data = cookies)
```
:::

:::

:::::

---

## Interpreting linear regression output

::::: columns
::: {.column width="50%"}

"A one unit increase in X is associated with a beta 1 increase (or decrease) in Y, on average."

```{r}
#| echo: false
intercept_slope_highlight 
```

:::

::: {.column width="50%"}

We get some more detail doing this: 

```{r}
#| echo: true
# Compute the OLS regression model
ols_model <- lm(happiness ~ cookies, data = cookies)

summary(ols_model)
```

:::

:::::

---

## Interpreting linear regression output

::::: columns
::: {.column width="50%"}

"A one unit increase in X is associated with a beta 1 increase (or decrease) in Y, on average."

```{r}
#| echo: false
intercept_slope_highlight 
```

:::

::: {.column width="50%"}

We can also store the output in a data frame directly using the `broom` package. 

```{r}
#| echo: true
library(broom)

tidy(ols_model)
```

:::

:::::

# Statistical Inference for Linear Regression

---

## How does hypothesis testing work for linear regression? 

- Everything we've learned so far on hypothesis testing and statistical power also applies to linear regression

- In the last two sessions, our estimate was the difference between comedies and action movies. 

- This time, our estimate is the slope of the regression.

---

## Simulate a cookies and happiness population

Let's simulate a true effect world of cookie data with. 

```{r}
#| echo: true
set.seed(1234) # For reproducibility

# true happiness without any cookies
intercept <- 1

# effect of one cookie on happiness
slope <- 0.3

imaginary_cookies <- tibble(
  id = 1:1000000,
  cookies = sample(seq(1, 10, by = 1), size = 1000000, replace = TRUE),
  # the rnorm() function adds noise, i.e. some random error
  happiness = intercept + slope * cookies + rnorm(length(cookies), mean = 0, sd = 0.5)
)
```

--- 

## Generate a sampling distribution

Imagine we can ask a sample of 10 people on how happy they are and how many cookies they eat a day - let's draw many (1000) samples of 10 people and calculate a regression for each. 

```{r}
#| echo: false

# simulate different sampling distributions
get_sampling_distribution <- function(n_simulations = 1000, sample_size, data) {
  
regression_estimates <- c() # make an empty vector
  
for (i in 1:n_simulations) {
  # draw a sample 
  imaginary_sample <- imaginary_cookies |> 
    sample_n(sample_size)
  
  # run a regression on the sample 
  estimate <- lm(happiness ~ cookies, data = imaginary_sample) |> 
    tidy() |> 
    filter(term == "cookies") |> 
    pull(estimate)

  regression_estimates [i] <- estimate
}
  
  # Return a data frame with sample size
  return(tibble(sample_size = sample_size, estimate = regression_estimates))
}


# Define sample sizes to test
sample_sizes <- c(10, 30, 200)

# Run the function for each sample size and combine sampling_null
sampling <- bind_rows(lapply(sample_sizes, function(n) get_sampling_distribution(sample_size = n, data = imaginary_movies_null)))
```

```{r}
#| echo: true
#| eval: false
n_simulations <- 1000
regression_estimates <- c() # make an empty vector
sample_size <- 10

for (i in 1:n_simulations) {
  # draw a sample of 10 people
  imaginary_sample <- imaginary_cookies |> 
    sample_n(sample_size)
  
  # run a regression on the sample 
  estimate <- lm(happiness ~ cookies, data = imaginary_sample) |> 
    tidy() |> 
    filter(term == "cookies") |> 
    pull(estimate)

  regression_estimates [i] <- estimate
}
```

---

## According to the Central Limit Theorem, what do we expect the sampling distribution to look like?

---

## Sampling distribution

1. The sampling distribution of estimates approximates a normal distribution.

:::: r-stack

:::{.fragment .fade-out fragment-index="1"}

```{r}
# Plot histograms with fading effect
ggplot(sampling |> 
         filter(sample_size == 10), 
       aes(x = estimate, fill = factor(sample_size))) +
  geom_histogram(aes(y = ..density..), 
                 position = "identity", 
                 color = "black", 
                 alpha = 0.3) +
  scale_fill_viridis_d(option = "inferno") +
  labs(title = "Sampling distribution of regression slopes",
       x = "Estimate",
       y = "Density",
       fill = "Sample Size",
       color = "Sample Size") +
  theme_minimal()
```
:::

:::{.fragment .fade-in fragment-index="3"}
```{r}
# Compute mean and SD for each sample size
summary_stats <- sampling %>%
  group_by(sample_size) %>%
  summarise(mean_estimate = mean(estimate), 
            sd_estimate = sd(estimate)) 

# Compute mean and standard deviation of the simulated differences
estimate_mean <- summary_stats |> filter(sample_size == 10) |> pull(mean_estimate)
estimate_sd <- summary_stats |> filter(sample_size == 10) |> pull(sd_estimate)

# Plot histogram with normal curve
# Plot histograms with fading effect

ggplot(sampling |> 
         filter(sample_size == 10), 
       aes(x = estimate, fill = factor(sample_size))) +
  geom_histogram(aes(y = ..density..), 
                 position = "identity", 
                 color = "black", 
                 alpha = 0.3) +
    stat_function(fun = dnorm, args = list(mean = estimate_mean, sd = estimate_sd), 
                color = "red", linetype = "dashed", size = 1) + # Normal distribution curve
  scale_fill_viridis_d(option = "inferno") +
  labs(title = "Sampling distribution of regression slopes",
       x = "Estimate",
       y = "Density",
       fill = "Sample Size",
       color = "Sample Size") +
  theme_minimal() +
  scale_x_continuous(limits = c(0.15, 0.45))

```
:::

:::: 

---

## Sampling distribution

2. The sampling distribution gets narrower with a larger sample size

:::: r-stack

:::{.fragment .fade-out fragment-index="1"}
```{r}
# Compute mean and SD for each sample size
summary_stats <- sampling %>%
  group_by(sample_size) %>%
  summarise(mean_estimate = mean(estimate), 
            sd_estimate = sd(estimate)) 

# Compute mean and standard deviation of the simulated differences
estimate_mean <- summary_stats |> filter(sample_size == 10) |> pull(mean_estimate)
estimate_sd <- summary_stats |> filter(sample_size == 10) |> pull(sd_estimate)

# Plot histogram with normal curve
# Plot histograms with fading effect

ggplot(sampling |> 
         filter(sample_size == 10), 
       aes(x = estimate, fill = factor(sample_size))) +
  geom_histogram(aes(y = ..density..), 
                 position = "identity", 
                 color = "black", 
                 alpha = 0.3) +
    stat_function(fun = dnorm, args = list(mean = estimate_mean, sd = estimate_sd), 
                color = "red", linetype = "dashed", size = 1) + # Normal distribution curve
  scale_fill_viridis_d(option = "inferno") +
  labs(title = "Sampling distribution of regression slopes",
       x = "Estimate",
       y = "Density",
       fill = "Sample Size",
       color = "Sample Size") +
  theme_minimal() +
  scale_x_continuous(limits = c(0.15, 0.45))

```
:::

:::{.fragment .fade-in fragment-index="1"}

```{r}
# Compute mean and standard deviation of the simulated differences
estimate_mean_30 <- summary_stats |> filter(sample_size == 30) |> pull(mean_estimate)
estimate_sd_30 <- summary_stats |> filter(sample_size == 30) |> pull(sd_estimate)

# Plot histogram with normal curve
# Plot histograms with fading effect

ggplot(sampling |> 
         filter(sample_size == 30), 
       aes(x = estimate, fill = factor(sample_size))) +
  geom_histogram(aes(y = ..density..), 
                 position = "identity", 
                 color = "black", 
                 alpha = 0.3) +
    stat_function(fun = dnorm, args = list(mean = estimate_mean, sd = estimate_sd), 
                color = "red", linetype = "dashed", size = 1, alpha = 0.3) + # Normal distribution curve
      stat_function(fun = dnorm, args = list(mean = estimate_mean_30, sd = estimate_sd_30), 
                color = "red", linetype = "dashed", size = 1) + # Normal distribution curve
  scale_fill_viridis_d(option = "inferno") +
  labs(title = "Sampling distribution of regression slopes",
       x = "Estimate",
       y = "Density",
       fill = "Sample Size",
       color = "Sample Size") +
  theme_minimal() +
  scale_x_continuous(limits = c(0.15, 0.45))
```
::: 

::::

---

## Sampling distribution

2. The sampling distribution gets narrower with a larger sample size

```{r}
# Compute mean and standard deviation of the simulated differences
estimate_mean_200 <- summary_stats |> filter(sample_size == 200) |> pull(mean_estimate)
estimate_sd_200 <- summary_stats |> filter(sample_size == 200) |> pull(sd_estimate)

# Plot histogram with normal curve
# Plot histograms with fading effect
ggplot(sampling |> 
         filter(sample_size == 200), 
       aes(x = estimate, fill = factor(sample_size))) +
  geom_histogram(aes(y = ..density..), 
                 position = "identity", 
                 color = "black", 
                 alpha = 0.3) +
    stat_function(fun = dnorm, args = list(mean = estimate_mean, sd = estimate_sd), 
                color = "red", linetype = "dashed", size = 1, alpha = 0.3) + # Normal distribution curve
      stat_function(fun = dnorm, args = list(mean = estimate_mean_30, sd = estimate_sd_30), 
                color = "red", linetype = "dashed", size = 1, alpha = 0.3) + # Normal distribution curve
        stat_function(fun = dnorm, args = list(mean = estimate_mean_200, sd = estimate_sd_200), 
                color = "red", linetype = "dashed", size = 1) + # Normal distribution curve
  scale_fill_viridis_d(option = "inferno") +
  labs(title = "Sampling distribution of regression slopes",
       x = "Estimate",
       y = "Density",
       fill = "Sample Size",
       color = "Sample Size") +
  theme_minimal() +
  scale_x_continuous(limits = c(0.15, 0.45))
```

---

## Your turn: A power simulation for a regression analysis

Imagine we want to run a study to measure the association between cookies and happiness. 

<br>

We want to know how many participants we need to recruit to achieve a statistical power of 0.8, assuming an effect of 0.1. 

<br>

The final output should be a graph with the power for different sample sizes. 

::: {.callout-note}
This task is definitely a challenge. It's ok to get stuck. Make sure to use the guide on power simulation on the course website, and build on the functions on the next slides.
:::

```{r}
countdown::countdown(
  minutes = 15,
  bottom = 0, 
  right = 0,
  # Fanfare when it's over
  play_sound = FALSE,
  color_border              = "#FFFFFF",
  color_text                = "#7aa81e",
  color_running_background  = "#7aa81e",
  color_running_text        = "#FFFFFF",
  color_finished_background = "#ffa07a",
  color_finished_text       = "#FFFFFF",
  font_size = "1em",
  start_immediately = TRUE
  )
```

---

## Your turn: A power simulation for a regression analysis

```{r}
#| echo: true
generate_sample <- function(sample_size){
  
  # set regression parameters
  intercept <- 1
  slope <- 0.1
  
  sample <- tibble(
    id = 1: sample_size,
    cookies = sample(seq(1, 10, by = 1), size = sample_size, replace = TRUE),
    # the rnorm() function adds noise, i.e. some random error
    happiness = intercept + slope * cookies + rnorm(length(cookies), mean = 0, sd = 1)
  )
  
  return(sample)
}

# test
# generate_sample(sample_size = 100)
```

---

## Your turn: A power simulation for a regression analysis

```{r}
#| echo: true
calculate_regression <- function(sample){
  
  # run a regression on the sample 
  p.value <- lm(happiness ~ cookies, data = sample) |> 
    tidy() |> 
    filter(term == "cookies") |> 
    pull(p.value)
  
  return(p.value)
}

# test
# test_sample <- generate_sample(sample_size = 100)
# calculate_regression(sample = test_sample)
```

---

## Your turn: A power simulation for a regression analysis

```{r}
#| echo: true
generate_samples <- function(n_simulations, sample_size) {
  
  # Make an empty vector
  p.values <- numeric(n_simulations)
  
  for (i in 1:n_simulations) {
    # Draw a sample with the specified size
    sample <- generate_sample(sample_size) 
    
    # Get an estimate
    p.values[i] <-  calculate_regression(sample)
  }
  
  return(p.values)
}
# test
# generate_samples(n_simulations = 100, sample_size = 10)
```

---

## Solution 

First, we need a function to calculate power

```{r}
#| echo: true
calculate_power <- function(p.values){
  
  # get statistical power 
  power <- data.frame(p.values) |> 
    mutate(significant = ifelse(p.values <= 0.5, TRUE, FALSE)) |> 
    summarize(share_significant = sum(significant) / n()) |> 
    pull(share_significant)
  
  return(power)
}
# test
# some_p.values <- generate_samples(n_simulations = 100, sample_size = 10)
# calculate_power(p.values = some_p.values)
```

---

## Solution 

We can then put it all together in a power simulation function. This function generates 1000 samples, calculates the p-value for the analysis on each sample, and calculates the power.

```{r}
#| echo: true
power_simulation <- function(sample_size, n_simulations = 1000) {
  
  # Generate multiple samples and compute estimates
  sampled_p.values <- generate_samples(n_simulations, sample_size)
  
  # Calculate statistical power
  power <- calculate_power(sampled_p.values)
  
  # Return results
  return(tibble(
    sample_size = sample_size,
    n_simulations = n_simulations,
    estimated_power = power
  ))
}
# test
# power_simulation(sample_size = 30, n_simulations = 100)
```

---

## Solution 

Finally we can use the function to run a power analysis for different sample sizes

```{r}
#| echo: true
sample_sizes <- c(10, 30, 50, 200)

# make an empty data frame
power_data <- tibble()

for (i in sample_sizes) {
  # run power simulation
  power <- power_simulation(sample_size = i, n_simulations = 1000)
  
  power_data <- bind_rows(power_data, power)
}
```

---

## Solution 

We can then plot the power curve

```{r}
#| echo: true
ggplot(power_data, 
       aes(x = sample_size, y = estimated_power)) +
  geom_point(color = 'red', size = 1.5) +
  geom_line(color = 'red', size = 1) + 
  # add a horizontal line at 80%
  geom_hline(aes(yintercept = .8), linetype = 'dashed') + 
  # Prettify!
  theme_minimal() + 
  scale_y_continuous(labels = scales::percent, limits = c(0,1)) + 
  labs(title = "Power Simulation for the effect of Cookies on Happiness",
       x = 'Sample Size', y = 'Power')
```

---

That's it for today :)








